"""Code task generator and evaluator using code execution"""

import asyncio
import gc
import json
import logging
import os
import re
import signal
import sys
import tempfile

from datasets import load_dataset

sys.path.insert(0, '/app')
from models import Challenge
from utils import (
    BASE_IMPORTS,
    generate_function_wrapper,
    compare_stdout_results,
)

logger = logging.getLogger("i3_code")
handler = logging.StreamHandler(sys.stderr)
log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
date_format = "%Y-%m-%d %H:%M:%S"
handler.setFormatter(logging.Formatter(fmt=log_format, datefmt=date_format))
logger.addHandler(handler)
logger.setLevel(os.environ.get("I3_CODE_LOG_LEVEL", "INFO"))

# Use original simple instruction prompt
INSTRUCTION_PROMPT = "Solve the programming task below in a Python markdown code block."

# Default timeout per test case (seconds)
DEFAULT_TEST_TIMEOUT = 20

# Memory limit per subprocess in MB (prevent container OOM)
SUBPROCESS_MEMORY_LIMIT_MB = 1024

# Global semaphore for test concurrency control (lazy initialization)
_GLOBAL_TEST_SEMAPHORE = None

def _get_semaphore():
    """Get or create global test semaphore"""
    global _GLOBAL_TEST_SEMAPHORE
    if _GLOBAL_TEST_SEMAPHORE is None:
        _GLOBAL_TEST_SEMAPHORE = asyncio.Semaphore(5)
    return _GLOBAL_TEST_SEMAPHORE



def extract_code_from_markdown(text: str) -> str:
    """Extract Python code from markdown code blocks."""
    # Try to find ```python blocks first
    pattern = r"```python\s*(.*?)```"
    matches = re.findall(pattern, text, re.DOTALL)
    if matches:
        return matches[0].strip()
    
    # Fall back to any ``` blocks
    pattern = r"```\s*(.*?)```"
    matches = re.findall(pattern, text, re.DOTALL)
    if matches:
        return matches[0].strip()
    
    # If no code blocks, return the text as-is
    return text.strip()




class CodeTask:
    """Code task generator and evaluator using INTELLECT-3-RL dataset"""
    
    def __init__(
        self,
        dataset_name: str = "PrimeIntellect/INTELLECT-3-RL",
        dataset_subset: str = "code",
        dataset_split: str = "train",
        dataset_shuffle: bool = False,
        difficulty_key: str = "avg@8_qwen3_4b_instruct_2507",
        min_avg_reward: float = 0.0,
        max_avg_reward: float = 1.0,
    ):
        """
        Initialize CodeTask with dataset configuration
        
        Args:
            dataset_name: HuggingFace dataset name
            dataset_subset: Dataset subset to use
            dataset_split: Dataset split (train/test/validation)
            dataset_shuffle: Whether to shuffle the dataset
            difficulty_key: Key for filtering by difficulty
            min_avg_reward: Minimum average reward filter
            max_avg_reward: Maximum average reward filter
        """
        logger.info(f"Loading dataset: {dataset_name}/{dataset_subset} split={dataset_split}")
        
        # Load and filter dataset
        def process_example(x):
            info = json.loads(x["info"])
            tests = json.loads(info["tests"])
            
            # Store inputs/outputs as JSON strings (one level of encoding)
            # This matches the original project's data format
            inputs = [json.dumps(tests["inputs"][i]) for i in range(len(tests["inputs"]))]
            outputs = [json.dumps(tests["outputs"][i]) for i in range(len(tests["outputs"]))]
            
            # Extract fn_name if present
            fn_name = tests.get("fn_name", "") or ""
            
            # Rebuild tests dict with encoded strings
            encoded_tests = {
                "inputs": inputs,
                "outputs": outputs,
                "fn_name": fn_name
            }
            
            return {
                "question": INSTRUCTION_PROMPT + "\n\n" + x["question"],
                "tests": json.dumps(encoded_tests),  # Store as JSON string
                "source": info.get("source", "")
            }
        
        self.dataset = (
            load_dataset(dataset_name, dataset_subset, split=dataset_split)
            .filter(lambda x: min_avg_reward <= x.get(difficulty_key, 0) <= max_avg_reward)
            .map(process_example)
        )
        
        if dataset_shuffle:
            self.dataset = self.dataset.shuffle(seed=42)
        
        logger.info(f"Dataset loaded: {len(self.dataset)} examples")
    
    async def generate(self, task_id: int = None) -> Challenge:
        """
        Generate a code task challenge
        
        Args:
            task_id: Optional task ID for deterministic selection.
                     If provided, used as index into dataset.
                     If not provided, random sample is selected.
        """
        if task_id is not None:
            # Use task_id as index for deterministic selection
            idx = task_id % len(self.dataset)
            sample = self.dataset[idx]
        else:
            # Random selection
            import random
            idx = random.randint(0, len(self.dataset) - 1)
            sample = self.dataset[idx]
        
        return Challenge(
            env="code",
            prompt=sample["question"],
            extra={
                "tests": sample["tests"],
                "source": sample.get("source", ""),
                "task_id": task_id,
                "dataset_index": idx
            }
        )
    
    async def evaluate(self, response: str, challenge: Challenge, timeout: int = DEFAULT_TEST_TIMEOUT) -> tuple[float, str]:
        """
        Evaluate code response by running test cases in parallel using stdin/stdout
        
        Args:
            response: Model response containing code
            challenge: Original challenge with test cases
            timeout: Timeout per test case in seconds
        
        Returns:
            Tuple of (score, test_result):
                - score: 1.0 if all tests pass, else 0.0
                - test_result: String in format "passed/total" (e.g., "7/15")
        """
        # Log evaluation start
        task_id = challenge.extra.get("task_id")
        model = challenge.extra.get("model", "N/A")
        base_url = challenge.extra.get("base_url", "N/A")
        logger.info(
            f"Evaluation start: task_id={task_id}, model={model}, base_url={base_url}"
        )
        
        tests_str = challenge.extra.get("tests", "")
        if not tests_str:
            logger.warning("No tests provided")
            return 0.0, "0/0"
        
        # Extract code from response
        code = extract_code_from_markdown(response)
        if not code:
            logger.warning("No code found in response")
            return 0.0, "0/0"
        
        logger.debug(f"Extracted code:\n{code}")
        
        # Parse tests
        try:
            tests = json.loads(tests_str)
        except Exception as e:
            logger.error(f"Failed to parse tests: {e}")
            return 0.0, "0/0"
        
        # Extract test data (inputs and outputs are JSON strings)
        inputs = tests.get("inputs", [])
        outputs = tests.get("outputs", [])
        fn_name = tests.get("fn_name", "")
        
        if not inputs or not outputs:
            logger.warning("No test inputs/outputs found")
            return 0.0, "0/0"
        
        if len(inputs) != len(outputs):
            logger.error(f"Mismatch: {len(inputs)} inputs vs {len(outputs)} outputs")
            return 0.0, f"0/{len(inputs)}"
        
        # Determine execution mode based on fn_name
        use_function_mode = bool(fn_name and fn_name.strip())
        
        # Run tests with limited concurrency to prevent memory exhaustion
        total = len(inputs)
        results = []
        
        async def run_test_with_semaphore(i):
            """Run a single test with global semaphore to limit concurrency"""
            async with _get_semaphore():
                try:
                    # Parse input and output (they are JSON strings)
                    test_input = json.loads(inputs[i])
                    expected_output = json.loads(outputs[i])

                    if use_function_mode:
                        # Function call mode - pass JSON string for expected_output
                        # to match original project's comparison logic
                        result = await self._run_function_test(
                            code=code,
                            fn_name=fn_name,
                            test_input=test_input,
                            expected_output=outputs[i],  # Pass JSON string
                            timeout=timeout,
                            test_index=i
                        )
                        if not result:
                            print(f"Testcase {i} failed.")
                            print(f"input: {test_input}")
                            print(f"output: {outputs[i]}")
                    else:
                        # Stdin/stdout mode
                        if isinstance(test_input, str):
                            stdin_input = test_input
                        else:
                            stdin_input = str(test_input)
                        
                        result = await self._run_stdin_test(
                            code=code,
                            stdin_input=stdin_input,
                            expected_output=expected_output,
                            timeout=timeout,
                            test_index=i
                        )
                        if not result:
                            print(f"Testcase {i} failed.")
                            print(f"input: {stdin_input}")
                            print(f"output: {expected_output}")
                    return result
                        
                except Exception as e:
                    logger.debug(f"Test {i}: Failed to prepare - {e}")
                    return None
        
        # Execute all tests with controlled concurrency
        tasks = [run_test_with_semaphore(i) for i in range(total)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Count passed tests
        passed = sum(1 for r in results if r is True)
        score = 1.0 if passed == total else 0.0
        
        test_result = f"{passed}/{total}"
        logger.info(
            f"Evaluation complete: task_id={task_id}, model={model}, "
            f"base_url={base_url}, {test_result}, score={score}"
        )
        
        return score, test_result
    
    async def _run_test_subprocess(
        self,
        code: str,
        timeout: int,
        test_index: int,
        stdin_input: str = None
    ) -> tuple[asyncio.subprocess.Process, bytes, bytes]:
        """
        Run code in subprocess with memory monitoring
        
        Returns:
            (process, stdout, stderr)
        """
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(f"{BASE_IMPORTS}\n{code}")
            temp_file = f.name
        
        process = None
        try:
            def _preexec():
                # Run subprocess in its own process group/session so that
                # group-level kills never affect the env process itself.
                try:
                    if hasattr(os, "setsid"):
                        os.setsid()
                except Exception:
                    # Best-effort isolation; continue even if setsid fails.
                    pass
                if hasattr(os, "setrlimit"):
                    self._set_process_limits()

            process = await asyncio.create_subprocess_exec(
                'python3', temp_file,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                preexec_fn=_preexec
            )
            
            monitor_task = asyncio.create_task(self._monitor_process_memory(process, test_index))
            
            try:
                stdin_bytes = stdin_input.encode('utf-8') if stdin_input else None
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(input=stdin_bytes),
                    timeout=timeout
                )
                return process, stdout, stderr
            finally:
                monitor_task.cancel()
                try:
                    await monitor_task
                except asyncio.CancelledError:
                    pass
        finally:
            await self._cleanup_process(process, test_index)
            try:
                os.unlink(temp_file)
            except (OSError, FileNotFoundError):
                pass
            gc.collect()
    
    async def _cleanup_process(self, process, test_index: int):
        """Clean up subprocess aggressively"""
        if not process:
            return
        
        try:
            pgid = os.getpgid(process.pid)
            current_pgid = os.getpgid(0)
            # Never kill our own process group (which would kill the env)
            if pgid != current_pgid:
                os.killpg(pgid, signal.SIGKILL)
        except (ProcessLookupError, PermissionError, AttributeError, OSError):
            pass
        
        try:
            process.kill()
        except (ProcessLookupError, PermissionError):
            pass
        
        try:
            await asyncio.wait_for(process.wait(), timeout=1.0)
        except asyncio.TimeoutError:
            logger.warning(f"Test {test_index}: Cleanup timeout for PID {process.pid}")
        
        # Extra cleanup: ensure no zombie processes
        try:
            import subprocess
            subprocess.run(['pkill', '-9', '-P', str(process.pid)], capture_output=True, timeout=1)
        except:
            pass
    
    async def _run_stdin_test(
        self,
        code: str,
        stdin_input: str,
        expected_output,
        timeout: int,
        test_index: int
    ) -> bool:
        """Run stdin/stdout test case"""
        try:
            process, stdout, stderr = await self._run_test_subprocess(
                code, timeout, test_index, stdin_input
            )
            
            if process.returncode == -9:
                logger.warning(f"Test {test_index}: Killed by memory monitor")
                return False
            
            if process.returncode != 0:
                logger.debug(f"Test {test_index}: Exit code {process.returncode}")
                return False
            
            actual = stdout.decode('utf-8', errors='ignore')
            expected = str(expected_output) if not isinstance(expected_output, str) else expected_output
            
            if compare_stdout_results(actual, expected):
                return True
            
            logger.debug(f"Test {test_index}: Output mismatch")
            return False
            
        except asyncio.TimeoutError:
            logger.debug(f"Test {test_index}: Timeout")
            return False
        except Exception as e:
            logger.debug(f"Test {test_index}: Exception - {e}")
            return False
    
    async def _run_function_test(
        self,
        code: str,
        fn_name: str,
        test_input,
        expected_output,
        timeout: int,
        test_index: int
    ) -> bool:
        """Run function-based test case"""
        try:
            # Prepare wrapper code
            test_input_str = "\n".join(str(k) for k in test_input) if isinstance(test_input, list) else str(test_input)
            wrapper_code = generate_function_wrapper(f"{BASE_IMPORTS}\n{code}", fn_name, test_input_str)
            
            process, stdout, stderr = await self._run_test_subprocess(
                wrapper_code.replace(f"{BASE_IMPORTS}\n", ""),  # Remove duplicate BASE_IMPORTS
                timeout, test_index
            )
            
            if process.returncode == -9:
                logger.warning(f"Test {test_index}: Killed by memory monitor")
                return False
            
            if process.returncode != 0:
                logger.debug(f"Test {test_index}: Exit code {process.returncode}")
                return False
            
            # Parse and compare results
            result_data = json.loads(stdout.decode('utf-8', errors='ignore').strip())
            if not result_data.get("success", False):
                logger.debug(f"Test {test_index}: Execution failed")
                return False
            
            exec_outputs = result_data["result"]
            test_case_outputs = json.loads(expected_output)
            if isinstance(test_case_outputs, str):
                try:
                    test_case_outputs = json.loads(test_case_outputs)
                except (json.JSONDecodeError, TypeError):
                    pass
            
            # Comparison logic
            if isinstance(exec_outputs, tuple):
                exec_outputs = list(exec_outputs)
            
            print(f"# exec_outputs #: {exec_outputs}")
            print(f"# test_case_outputs #: {test_case_outputs}")
            if exec_outputs == test_case_outputs:
                return True
            if isinstance(test_case_outputs, list) and exec_outputs == test_case_outputs[0]:
                return True
            
            try:
                if isinstance(exec_outputs[0], tuple):
                    exec_outputs = [list(x) for x in exec_outputs]
                    if exec_outputs == test_case_outputs[0]:
                        return True
            except:
                pass
            
            logger.debug(f"Test {test_index}: Result mismatch")
            return False
            
        except asyncio.TimeoutError:
            logger.debug(f"Test {test_index}: Timeout")
            return False
        except (json.JSONDecodeError, KeyError) as e:
            logger.debug(f"Test {test_index}: Parse error - {e}")
            return False
        except Exception as e:
            logger.debug(f"Test {test_index}: Exception - {e}")
            return False
    
    async def _monitor_process_memory(self, process, test_index: int):
        """Monitor subprocess memory and kill if exceeds limit"""
        try:
            import psutil
            # Reduced initial delay from 0.2s to 0.1s for faster response
            await asyncio.sleep(0.1)
            
            try:
                proc = psutil.Process(process.pid)
            except psutil.NoSuchProcess:
                return
            
            while True:
                try:
                    if not proc.is_running():
                        return
                    
                    # Monitor both RSS and VMS for more accurate tracking
                    mem_info = proc.memory_info()
                    rss_mb = mem_info.rss / 1024 / 1024
                    vms_mb = mem_info.vms / 1024 / 1024
                    
                    # Use RSS as primary metric, but warn on VMS growth
                    if rss_mb > SUBPROCESS_MEMORY_LIMIT_MB:
                        logger.debug(
                            f"Test {test_index}: Memory limit exceeded - "
                            f"PID={process.pid} RSS={rss_mb:.1f}MB > {SUBPROCESS_MEMORY_LIMIT_MB}MB"
                        )
                        proc.kill()
                        return
                    
                    # Also warn if VMS is significantly higher
                    if vms_mb > SUBPROCESS_MEMORY_LIMIT_MB * 1.5:
                        logger.debug(
                            f"Test {test_index}: High VMS - "
                            f"PID={process.pid} VMS={vms_mb:.1f}MB (RSS={rss_mb:.1f}MB)"
                        )
                    
                    # Reduced polling interval from 0.2s to 0.1s for tighter monitoring
                    await asyncio.sleep(0.1)
                    
                except psutil.NoSuchProcess:
                    return
                except Exception as e:
                    logger.error(f"Test {test_index}: Monitor error: {e}")
                    return
                    
        except ImportError:
            logger.debug("psutil not available, memory monitoring disabled")
        except Exception as e:
            logger.error(f"Test {test_index}: Monitor failed to start: {e}")
    
    @staticmethod
    def _set_process_limits():
        """Set resource limits for subprocess"""
        try:
            import resource
            resource.setrlimit(resource.RLIMIT_CPU, (60, 60))
            resource.setrlimit(resource.RLIMIT_NOFILE, (256, 256))
        except (ImportError, OSError):
            pass